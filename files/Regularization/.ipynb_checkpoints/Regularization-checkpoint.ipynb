{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias and Variance: \n",
    "\n",
    "Two common problems with model. \n",
    "\n",
    "Over-fitting - the model fits the training data but fails to establish a relationship among the features and perfroms poorly on the test data. \n",
    "\n",
    "Underfitting - the model neither fits the training data nor the test data. \n",
    "    \n",
    "<img src =\"various_fits.png\", width = 400, height = 300> \n",
    "\n",
    "Bias measures how far we are from the actual value on an average. \n",
    "\n",
    "Variance measures spread of the target values. \n",
    "\n",
    "<img src =\"bias_variance.png\", width = 400, height = 300> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is a technique to overcome both overfitting or underfitting of the data. \n",
    "\n",
    "If $L$ represents the loss function then our goal is to \n",
    "\n",
    "minimize (L + regularization term). The regularization term is know as the penalty term. Below we have different regularization with the norms:\n",
    "\n",
    "L1 norm: $\\lambda ||w||_1 = \\lambda   \\sum |w_j| $\n",
    "\n",
    "L2 norm: $\\lambda ||w||_2^2  = \\lambda  \\sum w^2_j $\n",
    "\n",
    "where $w$ is a matrix that contains coefficients for the different features and $\\lambda$ is a regularization parameter. \n",
    "\n",
    "Least Absolute Shrinkage and Selection Operator (LASSO) regression  uses the L1 regularization. This is used when we have more number of features. \n",
    "\n",
    "Ridge regression uses L2 regularization. This is used to prevent multicollinearity. \n",
    "\n",
    "The value of $\\alpha$ controls the penalty term. If $\\alpha$ is very high, then the penalty is high and thus, the magnitude of the coefficients will be small.\n",
    "\n",
    "#### Important Note: In sklearn for Lasso, Ridge and Elastic, alpha is same as $\\lambda$ in the regularization equation. \n",
    "\n",
    "#### Hyperparamters are the parameters that the user has to provide manually. Here $\\alpha$ is a hyperparameter. \n",
    "\n",
    "Elastic net regression is a combination of both L1 and L2 regularization. \n",
    "\n",
    "$min( L + \\lambda_1 ||w||_1 + \\lambda_2 ||w||_2^2) $\n",
    "\n",
    "$\\alpha = \\lambda_1 + \\lambda_2 $ and \n",
    "\n",
    "$l1\\_ratio = \\frac{\\lambda_1}{\\lambda_1 + \\lambda_2} $\n",
    "\n",
    "$l1\\_ratio = 1$ can only happen when $\\lambda_1 = 1$ and $\\lambda_2 = 0,$ this will result in Lasso regression.\n",
    "\n",
    "$l1\\_ratio = 0$ can only happen when $\\lambda_1 = 0$ and this will result in Ridge regression.\n",
    "\n",
    "For $l1\\_ratio$ between 0 and 1, we get Elastic net regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between model complexity and error\n",
    "\n",
    "Reference: http://www.frank-dieterle.de/phd/2_8_1.html\n",
    "\n",
    "\n",
    "<img src =\"model_complexity.png\", width = 300, height = 200> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = pd.read_csv('auto_mpg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['mpg', 'cylinder', 'displacement', 'horse power', 'weight',\n",
      "       'acceleration', 'model year', 'origin', 'car name'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(auto.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = auto[auto[\"horse power\"] != \"?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(392, 9)\n"
     ]
    }
   ],
   "source": [
    "print(auto.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "autox = auto[[\"displacement\", \"horse power\", \"weight\", \"acceleration\"]].copy(deep=True)\n",
    "autoy = auto[[\"mpg\"]].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "autox[\"displacement\"] = autox[\"displacement\"].astype(float)\n",
    "autox[\"horse power\"] = autox[\"horse power\"].astype(float)\n",
    "autox[\"weight\"] = autox[\"weight\"].astype(float)\n",
    "autox[\"accelaration\"] = autox[\"acceleration\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(autox, \\\n",
    "                                                    autoy, \\\n",
    "                                                    test_size=0.2, \\\n",
    "                                                    random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(313, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79,) (79, 1)\n",
      "18.773024487514057\n",
      "0.7123189217847542\n",
      "[-0.00121657 -0.0218214  -0.00568267  0.          0.        ] [42.8649594]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lassoReg = Lasso(alpha=0.05, normalize=True)\n",
    "\n",
    "lassoReg.fit(x_train,y_train)\n",
    "\n",
    "pred = lassoReg.predict(x_test)\n",
    "print(pred.shape, y_test.shape)\n",
    "\n",
    "# calculating mse\n",
    "\n",
    "mse = np.mean((pred - np.array(y_test).flatten())**2)\n",
    "print(mse)\n",
    "\n",
    "\n",
    "b = lassoReg.score(x_test,y_test) # returns the r-squared value\n",
    "print(b)\n",
    "print(lassoReg.coef_, lassoReg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7245311044437953\n",
      "[[-0.01324779 -0.05028545 -0.00428272 -0.09528115 -0.09528115]] [46.92147583]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "## training the model\n",
    "\n",
    "ridgeReg = Ridge(alpha=0.05, normalize=True)\n",
    "\n",
    "ridgeReg.fit(x_train,y_train)\n",
    "\n",
    "pred = ridgeReg.predict(x_test)\n",
    "\n",
    "mse = np.mean((y_test - pred)**2)\n",
    "rd= ridgeReg.score(x_test,y_test) # returns the r-squared value\n",
    "print(rd)\n",
    "print(ridgeReg.coef_, ridgeReg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79,)\n",
      "(79, 1)\n",
      "0.7253272822673765\n"
     ]
    }
   ],
   "source": [
    "# Elastic Net code\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "enReg = ElasticNet(alpha=1, l1_ratio=0.5, normalize=False)\n",
    "\n",
    "enReg.fit(x_train,y_train)\n",
    "\n",
    "pred_en = enReg.predict(x_test)\n",
    "print(pred_en.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# calculating mse\n",
    "\n",
    "mse = np.mean((pred_en.reshape(-1,1) - y_test)**2)\n",
    "\n",
    "# calculating r-squared from Elastic Net\n",
    "\n",
    "en = enReg.score(x_test,y_test) \n",
    "print(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing a value in a column \n",
    "\"\"\"\n",
    "df[\"col_name\"] = df[\"col_name\"].replace(value_to_be_replaced, new_value)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing a value in multiple columns with one value\n",
    "\n",
    "\"\"\"\n",
    "cols = [\"col1\", \"col2\", \"col3\"]\n",
    "\n",
    "df[\"cols\"] = df[\"cols\"].replace(value_to_be_replaced, new_value)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding r-squared\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, yhat) \n",
    "\"\"\"\n",
    "# here yhat is same as y predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In-class activity: For the auto_mpg data, can you pick alpha = 0.1, \n",
    "alpha = 0.15, alpha = 0.2 and run Lasso and Ridge regressions? \n",
    "Each regression, plot the coefficient values for each alpha. \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
