{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal Component Analysis (PCA) \n",
    "\n",
    "Is a dimensionality reduction technique that helps with feature extraction and data visualization. PCA is used to transform a higher dimensional space to a lower dimensional space. \n",
    "\n",
    "\n",
    "Advantages of PCA:\n",
    "\n",
    "1) Removes correlated features\n",
    "\n",
    "2) Reduces overfitting\n",
    "\n",
    "3) PCA improves model performance\n",
    "\n",
    "4) Improves visualization\n",
    "\n",
    "Disadvantages of PCA:\n",
    "\n",
    "1) Less interpretable. After PCA, the Principal components will be a linear combinations of features and will be hard to interpret and read.\n",
    "\n",
    "2) Data should be scaled before applying PCA otherwise feature with small variance will be neglected. \n",
    "\n",
    "\n",
    "\n",
    "1) Dimensionality - The number of features in a dataset.\n",
    "\n",
    "2) Correlation - the linear relationship between two features. \n",
    "\n",
    "3) Orthogonal - correlation between any pair of features is zero.\n",
    "\n",
    "4) Eigen vectors - $v$ is an eigen vector of A is $ A v = \\lambda v $ where $\\lambda$ is a scalar.\n",
    "\n",
    "PCA steps:\n",
    "\n",
    "1) Normalize the data\n",
    "\n",
    "2) Calculate the covariance matrix\n",
    "\n",
    "<img src=\"covariance1.png, width=300, height=200>\n",
    "\n",
    "3) Calculate the eigen values and eigen vectors\n",
    "\n",
    "$ det( A - \\lambda I)  = 0 $\n",
    "\n",
    "$ ( A - \\lambda I)v  = 0 $\n",
    "\n",
    "4) Choosing components - arrange eigen values from highest to lowest and choose the componenets with highest eigen values. \n",
    "\n",
    "5) Forming Principal components \n",
    "\n",
    "new points = feature vectors * scaled data\n",
    "\n",
    "new data is the matrix consisting the principal components, feature vector is the matrix with eigen vectors what we considered, scaled data is the scaled version of the original data.\n",
    "\n",
    "Where is PCA used:\n",
    "\n",
    "1) Facial recognition, computer vision and image compression\n",
    "\n",
    "2) Finding patterns in data mining, bioinformatics, psychology, finance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the eigen values of eigen vectors \n",
    "\n",
    "$ A =[ [2, -1], [-1, 2]] $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In-class activity: If A = [[ 1, 2], [4, 3]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform PCA on the below dataset\n",
    "\n",
    "<img src=\"matrixA.png\", width=300, height=200>\n",
    "\n",
    "this dataset has three features.\n",
    "\n",
    "The mean of the features is\n",
    "\n",
    "<img src=\"meanofA.png\", width=300, height=200>\n",
    "\n",
    "The covariance formula is\n",
    "\n",
    "<img src=\"conv_formula.png\", width=300, height=200>\n",
    "\n",
    "\n",
    "The covariance matrix is \n",
    "\n",
    "<img src=\"covariance1.png\", width=300, height=200>\n",
    "\n",
    "The original dataset along with the convariance \n",
    "\n",
    "<img src=\"con2.png\", width=300, height=200>\n",
    "\n",
    "From the covariance matrix we can say that:\n",
    "\n",
    "1) Art has the biggest variance and English has the least.\n",
    "\n",
    "2) The covariance between Math and English is 360, while the covariance between MAtha nd Art is 180. \n",
    "\n",
    "3) The covariance beteen Art and English is 0. \n",
    "\n",
    "Let's compute the eigen values of A\n",
    "\n",
    "<img src=\"eigen1.png\", width=300, height=200>\n",
    "\n",
    "<img src=\"eigen2.png\", width=400, height=300>\n",
    "\n",
    "<img src=\"eigen3.png\", width=300, height=200>\n",
    "\n",
    "<img src=\"eigen4.png\", width=300, height=200>\n",
    "\n",
    "<img src=\"eigen5.png\", width=300, height=200>\n",
    "\n",
    "<img src=\"eigen6.png\", width=300, height=200>\n",
    "\n",
    "<img src=\"eigen7.png\", width=300, height=200>\n",
    "\n",
    "<img src=\"eigen8.png\", width=300, height=200>\n",
    "\n",
    "With PCA, we transformed a three dimensional space to two-dimensional space.\n",
    "\n",
    "References:\n",
    "https://towardsdatascience.com/the-mathematics-behind-principal-component-analysis-fff2d7f4b643"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "# load dataset into Pandas DataFrame\n",
    "df = pd.read_csv(url, names=['sepal length','sepal width','petal length','petal width','target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the data to mean zero and variance 1\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "features = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
    "# Separating out the features\n",
    "x = df.loc[:, features].values\n",
    "# Separating out the target\n",
    "y = df.loc[:,['target']].values\n",
    "# Standardizing the features\n",
    "x = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using PCA to reduce the dimensionality from 4 to 2\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(principalDf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating the target values and the new principal components\n",
    "\n",
    "finalDf = pd.concat([principalDf, df[['target']]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "colors = ['r', 'g', 'b']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['target'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important measure\n",
    "\n",
    "Explained variance tells us how much inofrmation (variance) \n",
    "can be attributed to each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "This means that the first principal component contians 72.77% of the variance and the second principal componenet contains 23.03% of the variance. Together they contain 95.08% of the information.\n",
    "\n",
    "References:\n",
    "https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In-class activity:\n",
    "\n",
    "Apply PCA to the income dataset that you used for the last \n",
    "homework 2 and find the explained variance ratio for each component when \n",
    "n_components is 2 and when n-components is 3.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
