{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM loss\n",
    "\n",
    "Let's consider the below figure:\n",
    "\n",
    "<img src=\"svm1.png\", width = 300, height=200>\n",
    "\n",
    "SVM's goal is to maximize the margin, for this reason, SVM is called the Large Margin Classifier. \n",
    "\n",
    "In two dimensions, the equation of the line is given by $ y = w x + b.$ \n",
    "\n",
    "Since the decision boundary is the separator, we can assume that on the decision boundary $y=0$ and above the decision boundary, $y = 1$ and below the decision boundary the decision boundary, $y=-1.$ If we choose a point $x^{-}$ which is on or below $y=-1$ and if we choose a point $x^{+}$ which is on or above $y=1,$ then the distance between $x^{-}$ and $x^{+}$ will be a scalar multiple of $w$\n",
    "\n",
    "$ x^{+} - x^{-} = r * w$\n",
    "\n",
    "$ x^{+} = r * w + x^{-} $\n",
    "\n",
    "let's plug this in $w x^{+} + b = 1 $\n",
    "\n",
    "$w*(r * w + x^{-}) + b = 1 $\n",
    "\n",
    "$ r ||w||^2 + w x^{-} + b = 1 $ \n",
    "\n",
    "we know that $ w x^{-} + b = -1, $ plugging this in the above equation will result in\n",
    "\n",
    "$ r ||w||^2 - 1 = 1 $\n",
    "\n",
    "$ r ||w||^2 = 2 $\n",
    "\n",
    "$ r = \\frac{2}{||w||^2} $\n",
    "\n",
    "we want to maximize $\\frac{2}{||w||^2} $ \n",
    "\n",
    "or minimize $||w||^2.$\n",
    "\n",
    "The loss is minimizing $||w||^2 $ along with max of $\\{0, 1 - y_{true} *y_{predicted} \\}.$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different branches of ML are:\n",
    "\n",
    "<img src=\"types1.png\", width=300, height=200> \n",
    "\n",
    "References: https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reinforcement Learning \n",
    "\n",
    "Is a method to train agents so that they can take actions in an environment so that they achieve action(s) with maximum reward(s). \n",
    "\n",
    "<img src=\"agent_env.png\", width=400, height=300>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning\n",
    "\n",
    "Is a Reinforcement learning that follows Q-learning algorithm, the flow chart of the algorithm is\n",
    "\n",
    "<img src=\"q_learning_chart.png\", width=300, height=200> \n",
    "\n",
    "\n",
    "Let's consider an example\n",
    "\n",
    "<img src=\"q_ex1.png\", width=300, height=200> \n",
    "\n",
    "Our goal is to start from anywhere and go to 5. Let's build a graph with reward \n",
    "\n",
    "<img src=\"q_ex2.png\", width=300, height=200> \n",
    "\n",
    "In Q-learning we will have two matrices, R and Q. R is known as the reward matrix, Q is the quality matrix of state-action. Intially Q will be a zero a matrix. If we write the graph with the values into R, we get the following\n",
    "\n",
    "<img src=\"q_ex3.png\", width=300, height=200> \n",
    "\n",
    "and the Q matrix will be\n",
    "\n",
    "<img src=\"q_ex4.png\", width=300, height=200> \n",
    "\n",
    "Q-learning steps are:\n",
    "\n",
    "<img src=\"q_learning_algo.png\", width=500, height=400> \n",
    "\n",
    "\n",
    "We will use the following equation to update values in Q matrix\n",
    "\n",
    "<img src=\"q_ex5.png\", width=400, height=300> \n",
    "\n",
    "\n",
    "Let's find Q(1, 5). Different possible ways to get to state 5 are: 1 to 5, 4 to 5, or 5 to 5. The maximum reward is for Q(5, 1) = 100\n",
    "\n",
    "\n",
    "<img src=\"q_ex5_a.png\", width=400, height=300> \n",
    "\n",
    "Q(1, 5) will be 100\n",
    "\n",
    "We continue taking actions and update the Q matrix.\n",
    "\n",
    "\n",
    "References: http://mnemstudio.org/path-finding-q-learning-tutorial.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
